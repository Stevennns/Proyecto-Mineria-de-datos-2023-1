{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "\n",
    "import os\n",
    "def get_hashtags_text(x):\n",
    "    if pd.notna(x) and 'hashtags' in x and x['hashtags']:\n",
    "        hashtags = x['hashtags']\n",
    "        hashtags_text = [ht['text'] for ht in hashtags]\n",
    "        return hashtags_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_urls(x):\n",
    "    if pd.notna(x) and 'urls' in x and x['urls']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_user_location(x):\n",
    "    if pd.notna(x) and 'location' in x:\n",
    "        return x['location']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Leer el archivo JSON línea por línea y cargar los datos en una lista\n",
    "        datos_json = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if 'retweeted_status' in data and data['retweeted_status'] != None:\n",
    "                    data = data['retweeted_status']\n",
    "                datos_json.append(data)\n",
    "\n",
    "        df = pd.DataFrame(datos_json)\n",
    "\n",
    "        df_new['id'] = df['id']\n",
    "        df_new['lang'] = df['lang']\n",
    "        df_new['text'] = df['text']\n",
    "        df_new['favorite_count'] = df['favorite_count']\n",
    "        df_new['retweet_count'] = df['retweet_count']\n",
    "        df_new['possibly_sensitive'] = df['possibly_sensitive']\n",
    "        df_new['created_at'] = df['created_at']\n",
    "        df_new['is_quote_status'] = df['is_quote_status']\n",
    "        df_new['entities_hashtags_text'] = df['entities'].apply(get_hashtags_text)\n",
    "        df_new['entities_urls'] = df['entities'].apply(get_urls)\n",
    "        df_new['user_location'] = df['user'].apply(get_user_location)\n",
    "        \n",
    "        # Asignar 0 a las columnas 'reply_count' y 'quote_count' si no existen\n",
    "        df_new['reply_count'] = df.get('reply_count', 0)\n",
    "        df_new['quote_count'] = df.get('quote_count', 0)\n",
    "\n",
    "        # Verificar si hay al menos una imagen en el JSON\n",
    "        has_image = []\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                extended_entities = df['extended_entities'][i]\n",
    "                if 'media' in extended_entities and len(extended_entities['media']) > 0:\n",
    "                    has_image.append(True)\n",
    "                else:\n",
    "                    has_image.append(False)\n",
    "            except (KeyError, TypeError):\n",
    "                has_image.append(False)\n",
    "\n",
    "        df_new['has_image'] = has_image\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Se produjo una excepción: {e}')\n",
    "\n",
    "    # Retornar el DataFrame resultante\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(folder_path):\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos en la carpeta\n",
    "        json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "        for file_name in json_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = load_json(file_path)\n",
    "\n",
    "            # Extraer el nombre del archivo sin \"_final_data.json\"\n",
    "            json_name = file_name.replace('_final_data.json', '')\n",
    "\n",
    "            # Agregar columna con el nombre del JSON\n",
    "            df['json_name'] = json_name\n",
    "\n",
    "            # Concatenar el DataFrame al DataFrame total\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Se produjo una excepción: {e}')\n",
    "\n",
    "    # Retornar el DataFrame resultante\n",
    "    return df_all\n",
    "\n",
    "def read_tsv_folder(folder_path):\n",
    "    df_combined = pd.DataFrame()  # DataFrame combinado para almacenar los datos de todos los archivos TSV\n",
    "\n",
    "    # Obtener la lista de archivos en la carpeta\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # Leer cada archivo TSV y combinar los datos en el DataFrame combinado\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".tsv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "            df_combined = pd.concat([df_combined, df], ignore_index=True)\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "df_json = load_jsons_from_folder(\"RDATA/CrisisMMD_v2.0/json\")\n",
    "df_tsv = read_tsv_folder(\"RDATA/CrisisMMD_v2.0/annotations\")\n",
    "\n",
    "\n",
    "df = pd.merge(df_json, df_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "df.drop('tweet_id',axis=1,inplace=True)\n",
    "df_con_nan = df.copy()\n",
    "df =  df[df['text_human'].notna()]\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "df_con_nan[\"fecha\"] = pd.to_datetime(df_con_nan['created_at'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "df[\"fecha\"] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "#print(a[1000]<a[1])\n",
    "\n",
    "#df_con_nan\n",
    "#print(mdates.DateFormatter(df_con_nan['created_at']))\n",
    "df_con_nan = df_con_nan[df_con_nan['fecha'] >= '2017']\n",
    "\n",
    "\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "def foo(d):\n",
    "    fecha = d\n",
    "    fecha_objeto = datetime.strptime(fecha, \"%b %d, %Y\")\n",
    "    fecha_transformada = fecha_objeto.strftime(\"%Y-%m-%d\")\n",
    "    return fecha_transformada\n",
    "\n",
    "def convertir_fecha(fecha):\n",
    "    fecha_dt = datetime.strptime(fecha, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    fecha_formato = fecha_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return fecha_formato\n",
    "\n",
    "#print(foo(\"Oct 10, 2017\"))\n",
    "l = [\"Oct 10, 2017\", \"Aug 26, 2017\", \"Sep 6, 2017\", \"Sep 20, 2017\", \"Nov 13, 2017\", \"Sep 20, 2017\"]\n",
    "l1 = [\"10-10-2017\", \"26-08-2017\", \"06-09-2017\", \"20-09-2017\", \"13-11-2017\", \"20-09-2017\", \"20-05-2017\"]\n",
    "\n",
    "fechas = list(map(foo, l))\n",
    "#print(fechas)\n",
    "\n",
    "df_con_nan[\"tdd_temp\"] = 0  # Crear una nueva columna temporal\n",
    "\n",
    "i = 0\n",
    "for desastre in df_con_nan['json_name'].unique():\n",
    "    d = df_con_nan[df_con_nan['json_name'] == desastre]\n",
    "\n",
    "    AA = d[\"created_at\"]\n",
    "    BB = pd.to_datetime(l1[i], dayfirst=True, format=\"%d-%m-%Y\").tz_localize(pytz.UTC)\n",
    "    df_con_nan.loc[df_con_nan['json_name'] == desastre, \"tdd_temp\"] = (pd.to_datetime(AA, format=\"%a %b %d %H:%M:%S %z %Y\") - BB).dt.days\n",
    "    #print()\n",
    "    #print(df_con_nan[df_con_nan[\"json_name\"] == desastre][[\"tdd_temp\", \"created_at\"]])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "df_con_nan[\"tdd\"] = df_con_nan[\"tdd_temp\"]  # Asignar la columna temporal a la columna \"tdd\"\n",
    "df_con_nan.drop(\"tdd_temp\", axis=1, inplace=True)  # Eliminar la columna temporal\n",
    "df_con_nan = df_con_nan[df_con_nan[\"tdd\"] > -5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17544, 21)\n",
      "(15477, 20)\n"
     ]
    }
   ],
   "source": [
    "print(df_con_nan.shape)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "muertes = {\"mexico_earthquake\":369,\n",
    "       \"hurricane_harvey\":107, #39 desaparecidos\n",
    "       \"hurricane_irma\":134, #77 desaparecidos\n",
    "       \"iraq_iran_earthquake\":630,\n",
    "       \"hurricane_maria\":3059, #35 desaparecidos no sumados\n",
    "       \"srilanka_floods\":203,\n",
    "       \"california_wildfires\":44} #24 desaparecidos\n",
    "heridos = {\"mexico_earthquake\":7631,\n",
    "       \"hurricane_harvey\":39,\n",
    "       \"hurricane_irma\":77,\n",
    "       \"iraq_iran_earthquake\":9388,\n",
    "       \"hurricane_maria\":35,\n",
    "       \"srilanka_floods\":203,\n",
    "       \"california_wildfires\":46}\n",
    "viviendas ={\"mexico_earthquake\":150000,\n",
    "       \"hurricane_harvey\":246900, #entre varios estados de US\n",
    "       \"hurricane_irma\":423554, #sumado entre Cuba, PR y Florida\n",
    "       \"iraq_iran_earthquake\":70000,\n",
    "       \"hurricane_maria\":77643, # se estima entre este valor y 200,000\n",
    "       \"srilanka_floods\":203,\n",
    "       \"california_wildfires\":18000}\n",
    "perdida_economica = {\"mexico_earthquake\":8000,\n",
    "       \"hurricane_harvey\":125000,\n",
    "       \"hurricane_irma\":64760,\n",
    "       \"iraq_iran_earthquake\":6726,\n",
    "       \"hurricane_maria\":90000,\n",
    "       \"srilanka_floods\":203,\n",
    "       \"california_wildfires\":18000} #en millones de dolares\n",
    "\n",
    "afectados =  {\"mexico_earthquake\":12000000,\n",
    "       \"hurricane_harvey\":13300000,\n",
    "       \"hurricane_irma\":16800000,\n",
    "       \"iraq_iran_earthquake\":1800000,\n",
    "       \"hurricane_maria\":3300000,\n",
    "       \"srilanka_floods\":203,\n",
    "       \"california_wildfires\":400000} #no se sabe con exactitud\n",
    "\n",
    "magnitud = {} #hacer un calculo según los datos arriba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
