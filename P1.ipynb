{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hashtags_text(x):\n",
    "    if pd.notna(x) and 'hashtags' in x and x['hashtags']:\n",
    "        hashtags = x['hashtags']\n",
    "        hashtags_text = [ht['text'] for ht in hashtags]\n",
    "        return hashtags_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_urls(x):\n",
    "    if pd.notna(x) and 'urls' in x and x['urls']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_user_location(x):\n",
    "    if pd.notna(x) and 'location' in x:\n",
    "        return x['location']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Leer el archivo JSON línea por línea y cargar los datos en una lista\n",
    "        datos_json = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if 'retweeted_status' in data and data['retweeted_status'] != None:\n",
    "                    data = data['retweeted_status']\n",
    "                datos_json.append(data)\n",
    "\n",
    "        df = pd.DataFrame(datos_json)\n",
    "\n",
    "        df_new['id'] = df['id']\n",
    "        df_new['lang'] = df['lang']\n",
    "        df_new['text'] = df['text']\n",
    "        df_new['favorite_count'] = df['favorite_count']\n",
    "        df_new['retweet_count'] = df['retweet_count']\n",
    "        df_new['possibly_sensitive'] = df['possibly_sensitive']\n",
    "        df_new['created_at'] = df['created_at']\n",
    "        df_new['is_quote_status'] = df['is_quote_status']\n",
    "        df_new['entities_hashtags_text'] = df['entities'].apply(get_hashtags_text)\n",
    "        df_new['entities_urls'] = df['entities'].apply(get_urls)\n",
    "        df_new['user_location'] = df['user'].apply(get_user_location)\n",
    "        \n",
    "        # Asignar 0 a las columnas 'reply_count' y 'quote_count' si no existen\n",
    "        df_new['reply_count'] = df.get('reply_count', 0)\n",
    "        df_new['quote_count'] = df.get('quote_count', 0)\n",
    "\n",
    "        # Verificar si hay al menos una imagen en el JSON\n",
    "        has_image = []\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                extended_entities = df['extended_entities'][i]\n",
    "                if 'media' in extended_entities and len(extended_entities['media']) > 0:\n",
    "                    has_image.append(True)\n",
    "                else:\n",
    "                    has_image.append(False)\n",
    "            except (KeyError, TypeError):\n",
    "                has_image.append(False)\n",
    "\n",
    "        df_new['has_image'] = has_image\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Se produjo una excepción: {e}')\n",
    "\n",
    "    # Retornar el DataFrame resultante\n",
    "    return df_new\n",
    "\n",
    "def get_magnitud(df):\n",
    "    #print(mexico.shape)\n",
    "    weights = [0.2,0.2,0.3,0.05,0,0.05,0.05,0.15]\n",
    "\n",
    "    human= pd.DataFrame(data=np.zeros(8),\n",
    "                        columns=[\"proportion\"],\n",
    "                        index=[\"affected_individuals\",\n",
    "                                \"infrastructure_and_utility_damage\",\n",
    "                                \"injured_or_dead_people\",\n",
    "                                \"missing_or_found_people\",\n",
    "                                \"not_humanitarian\",\n",
    "                                \"other_relevant_information\",\n",
    "                                \"rescue_volunteering_or_donation_effort\",\n",
    "                                \"vehicle_damage\"]\n",
    "                        )\n",
    "\n",
    "    #print(human)\n",
    "    a = df[\"text_human\"].value_counts(normalize=True)\n",
    "    a = a.to_frame().add(human,fill_value=0)\n",
    "\n",
    "    #print(a)\n",
    "\n",
    "    return round(np.average(a=a[\"proportion\"],weights=weights),3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud Mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073\n"
     ]
    }
   ],
   "source": [
    "mexico_json = load_json(\"RDATA/CrisisMMD_v2.0/json/mexico_earthquake_final_data.json\")\n",
    "mexico_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/mexico_earthquake_final_data.tsv\")\n",
    "\n",
    "mexico = pd.merge(mexico_json, mexico_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "mexico.drop('tweet_id',axis=1,inplace=True)\n",
    "mexico =  mexico[mexico['text_human'].notna()]\n",
    "mexico = mexico.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(mexico)\n",
    "print(magnitud)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud iraq Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.107\n"
     ]
    }
   ],
   "source": [
    "iraq_json = load_json(\"RDATA/CrisisMMD_v2.0/json/iraq_iran_earthquake_final_data.json\")\n",
    "iraq_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/iraq_iran_earthquake_final_data.tsv\")\n",
    "\n",
    "iraq = pd.merge(iraq_json, iraq_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "iraq.drop('tweet_id',axis=1,inplace=True)\n",
    "iraq =  iraq[iraq['text_human'].notna()]\n",
    "iraq = iraq.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(iraq)\n",
    "print(magnitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.078\n"
     ]
    }
   ],
   "source": [
    "california_json = load_json(\"RDATA/CrisisMMD_v2.0/json/california_wildfires_final_data.json\")\n",
    "california_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/california_wildfires_final_data.tsv\")\n",
    "\n",
    "california = pd.merge(california_json, california_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "california.drop('tweet_id',axis=1,inplace=True)\n",
    "california =  california[california['text_human'].notna()]\n",
    "california = california.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(california)\n",
    "print(magnitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud Harvey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    }
   ],
   "source": [
    "harvey_json = load_json(\"RDATA/CrisisMMD_v2.0/json/hurricane_harvey_final_data.json\")\n",
    "harvey_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/hurricane_harvey_final_data.tsv\")\n",
    "\n",
    "harvey = pd.merge(harvey_json, harvey_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "harvey.drop('tweet_id',axis=1,inplace=True)\n",
    "harvey =  harvey[harvey['text_human'].notna()]\n",
    "harvey = harvey.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(harvey)\n",
    "print(magnitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    }
   ],
   "source": [
    "irma_json = load_json(\"RDATA/CrisisMMD_v2.0/json/hurricane_irma_final_data.json\")\n",
    "irma_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/hurricane_irma_final_data.tsv\")\n",
    "\n",
    "irma = pd.merge(irma_json, irma_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "irma.drop('tweet_id',axis=1,inplace=True)\n",
    "irma =  irma[irma['text_human'].notna()]\n",
    "irma = irma.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(irma)\n",
    "print(magnitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud Maria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046\n"
     ]
    }
   ],
   "source": [
    "maria_json = load_json(\"RDATA/CrisisMMD_v2.0/json/hurricane_maria_final_data.json\")\n",
    "maria_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/hurricane_maria_final_data.tsv\")\n",
    "\n",
    "maria = pd.merge(maria_json, maria_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "maria.drop('tweet_id',axis=1,inplace=True)\n",
    "maria =  maria[maria['text_human'].notna()]\n",
    "maria = maria.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(maria)\n",
    "print(magnitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitud Srilanka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "srilanka_json = load_json(\"RDATA/CrisisMMD_v2.0/json/srilanka_floods_final_data.json\")\n",
    "srilanka_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/srilanka_floods_final_data.tsv\")\n",
    "\n",
    "srilanka = pd.merge(srilanka_json, srilanka_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "srilanka.drop('tweet_id',axis=1,inplace=True)\n",
    "srilanka =  srilanka[srilanka['text_human'].notna()]\n",
    "srilanka = srilanka.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "magnitud = get_magnitud(srilanka)\n",
    "print(magnitud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
