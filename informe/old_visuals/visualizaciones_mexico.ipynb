{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los datos relacionado al Terremoto de mexico\n",
    "Se comienza con los imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen funciones necesarias para extraer los datos del archivo .json y .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags_text(x):\n",
    "    if pd.notna(x) and 'hashtags' in x and x['hashtags']:\n",
    "        hashtags = x['hashtags']\n",
    "        hashtags_text = [ht['text'] for ht in hashtags]\n",
    "        return hashtags_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_urls(x):\n",
    "    if pd.notna(x) and 'urls' in x and x['urls']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_user_location(x):\n",
    "    if pd.notna(x) and 'location' in x:\n",
    "        return x['location']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "  \n",
    "def load_json_old(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo JSON en un DataFrame de Pandas.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo JSON.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con los datos del archivo JSON.\n",
    "    \"\"\"\n",
    "    # Crear un nuevo DataFrame vacío\n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Leer el archivo JSON línea por línea y cargar los datos en una lista\n",
    "        datos_json = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                datos_json.append(data)\n",
    "\n",
    "        # Convertir la lista de datos en un DataFrame de Pandas\n",
    "        df = pd.DataFrame(datos_json)\n",
    "\n",
    "        # Actualizar las columnas del nuevo DataFrame con los datos válidos\n",
    "        df_new['id'] = df['id']\n",
    "        df_new['lang'] = df['lang']\n",
    "        df_new['text'] = df['text']\n",
    "        df_new['favorite_count'] = df['favorite_count']\n",
    "        df_new['retweet_count'] = df['retweet_count']\n",
    "        df_new['possibly_sensitive'] = df['possibly_sensitive']\n",
    "        df_new['created_at'] = df['created_at']\n",
    "        df_new['is_quote_status'] = df['is_quote_status']\n",
    "        df_new = df_new[df['retweeted_status'].isna()]\n",
    "        df_new['entities_hashtags_text'] = df['entities'].apply(get_hashtags_text)\n",
    "        df_new['entities_urls'] = df['entities'].apply(get_urls)\n",
    "        df_new['user_location'] = df['user'].apply(get_user_location)\n",
    "\n",
    "        # Manejar la excepción de KeyError para las columnas opcionales\n",
    "        try:\n",
    "            df_new['reply_count'] = df['reply_count']\n",
    "            df_new['quote_count'] = df['quote_count']\n",
    "        except KeyError as e:\n",
    "            print(f'Una o más columnas no existen en el archivo JSON: {e}')\n",
    "\n",
    "        # Eliminar las columnas \"user\" y \"entities\"\n",
    "        #df_new.drop(['user', 'entities'], axis=1, inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Se produjo una excepción: {e}')\n",
    "\n",
    "    # Retornar el DataFrame resultante\n",
    "    return df_new\n",
    "\n",
    "def load_json2(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo JSON en un DataFrame de Pandas.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo JSON.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con los datos del archivo JSON.\n",
    "    \"\"\"\n",
    "    # Leer el archivo JSON línea por línea y cargar los datos en una lista\n",
    "    datos_json = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if 'retweeted_status' in data and data['retweeted_status'] != None:\n",
    "                data = data['retweeted_status']\n",
    "            datos_json.append(data)\n",
    "\n",
    "    # Convertir la lista de datos en un DataFrame de Pandas\n",
    "    df = pd.DataFrame(datos_json)\n",
    "\n",
    "    # Crear un nuevo DataFrame con las columnas deseadas\n",
    "    df_new = df[['id','lang','text','favorite_count','quote_count','reply_count','retweet_count', 'possibly_sensitive','created_at',\n",
    "                    'entities', 'is_quote_status',\n",
    "                'user']]\n",
    "\n",
    "\n",
    "    # Filtrar las filas donde \"retweeted_status\" no sea None o NaN\n",
    "    #df_new = df_new[df['retweeted_status'].isna()]\n",
    "\n",
    "    # Extraer la información de los hashtags, URLs y ubicación de usuario y agregarla como nuevas columnas\n",
    "    df_new['entities_hashtags_text'] = df_new['entities'].apply(get_hashtags_text).apply(lambda x: x if x else None)\n",
    "    df_new['entities_urls'] = df_new['entities'].apply(get_urls)\n",
    "    df_new['user_location'] = df_new['user'].apply(get_user_location)\n",
    "\n",
    "    # Eliminar las columnas \"user\" y \"entities\"\n",
    "    df_new.drop(['user', 'entities'], axis=1, inplace=True)\n",
    "\n",
    "    # Retornar el DataFrame resultante\n",
    "    return df_new\n",
    "\n",
    "def load_json(file_path):\n",
    "  \n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Leer el archivo JSON línea por línea y cargar los datos en una lista\n",
    "        datos_json = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if 'retweeted_status' in data and data['retweeted_status'] != None:\n",
    "                    data = data['retweeted_status']\n",
    "                datos_json.append(data)\n",
    "\n",
    "      \n",
    "        df = pd.DataFrame(datos_json)\n",
    "\n",
    "        df_new['id'] = df['id']\n",
    "        df_new['lang'] = df['lang']\n",
    "        df_new['text'] = df['text']\n",
    "        df_new['favorite_count'] = df['favorite_count']\n",
    "        df_new['retweet_count'] = df['retweet_count']\n",
    "        df_new['possibly_sensitive'] = df['possibly_sensitive']\n",
    "        df_new['created_at'] = df['created_at']\n",
    "        df_new['is_quote_status'] = df['is_quote_status']\n",
    "        df_new['entities_hashtags_text'] = df['entities'].apply(get_hashtags_text)\n",
    "        df_new['entities_urls'] = df['entities'].apply(get_urls)\n",
    "        df_new['user_location'] = df['user'].apply(get_user_location)\n",
    "        try:\n",
    "            df_new['reply_count'] = df['reply_count']\n",
    "            df_new['quote_count'] = df['quote_count']\n",
    "        except KeyError as e:\n",
    "            print(f'Una o más columnas no existen en el archivo JSON: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'Se produjo una excepción: {e}')\n",
    "\n",
    "    # Retornar el DataFrame resultante\n",
    "    return df_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la dataframe sobre el mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_json = load_json(\"RDATA/CrisisMMD_v2.0/json/mexico_earthquake_final_data.json\")\n",
    "mexico_tsv = pd.read_table(\"RDATA/CrisisMMD_v2.0/annotations/mexico_earthquake_final_data.tsv\")\n",
    "\n",
    "mexico = pd.merge(mexico_json, mexico_tsv[['tweet_id', 'text_info', 'text_info_conf', 'text_human', 'text_human_conf']],\n",
    "                    left_on='id', right_on='tweet_id', how='left')\n",
    "mexico.drop('tweet_id',axis=1,inplace=True)\n",
    "mexico.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se revisa su tamaño y que cotiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = mexico.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only numerical columns\n",
    "numerical_columns = mexico.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# select only non-numerical columns\n",
    "categorical_columns = mexico.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "boolean_columns = mexico.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "print(numerical_columns)\n",
    "\n",
    "print(categorical_columns)\n",
    "\n",
    "print(boolean_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se revisa si hay ids duplicadas, y si existen, eliminar esas filas del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico[\"id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico = mexico.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "mexico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos no duplicados, se revisa la variedad de ciertas columnas. Si estás columnas solo retornan un único valor, se eliminan por redundancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = mexico[\"lang\"]\n",
    "\n",
    "wa = []\n",
    "\n",
    "for i in language:\n",
    "    if i not in wa:\n",
    "        wa.append(i)\n",
    "\n",
    "print(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite = mexico[\"favorite_count\"]\n",
    "RT = mexico[\"retweet_count\"]\n",
    "\n",
    "i = 0\n",
    "\n",
    "no_fav = []\n",
    "no_RT = []\n",
    "\n",
    "while i < len(favorite):\n",
    "    if favorite[i]== 0:\n",
    "        no_fav.append(\"NO\")\n",
    "    if RT[i] == 0:\n",
    "        no_RT.append(\"NO\")\n",
    "\n",
    "\n",
    "    i +=1\n",
    "\n",
    "print(len(favorite))\n",
    "print(len(no_fav))\n",
    "print(len(RT))\n",
    "print(len(no_RT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = mexico[\"reply_count\"]\n",
    "quotes = mexico[\"quote_count\"]\n",
    "\n",
    "i=0\n",
    "\n",
    "no_reply = []\n",
    "no_quote = []\n",
    "\n",
    "while i<len(reply):\n",
    "    if reply[i] == 0:\n",
    "        no_reply.append(\"NO\")\n",
    "    if quotes[i] == 0 :\n",
    "        no_quote.append(\"NO\")\n",
    "    i += 1\n",
    "\n",
    "print(len(reply))\n",
    "print(len(no_reply))\n",
    "print(len(quotes))\n",
    "print(len(no_quote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quote = mexico[\"is_quote_status\"]\n",
    "\n",
    "is_not_quote = []\n",
    "\n",
    "i = 0\n",
    "while i<len(is_quote):\n",
    "    if not is_quote[i]:\n",
    "        is_not_quote.append(\"AAA\")\n",
    "    i+=1\n",
    "\n",
    "print(len(is_quote))\n",
    "print(len(is_not_quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sensitive = mexico[\"possibly_sensitive\"]\n",
    "\n",
    "is_not_sensitive= []\n",
    "\n",
    "i = 0\n",
    "while i<len(is_sensitive):\n",
    "    if not is_sensitive[i]:\n",
    "        is_not_sensitive.append(\"AAA\")\n",
    "    i+=1\n",
    "\n",
    "print(len(is_sensitive))\n",
    "print(len(is_not_sensitive))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se descubre que todos los tweets son en inglés, por lo que se elimina la columna \"lang\". El resto de los datos presenta algo de variedad, por lo que se mantienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mexico = mexico.drop([\"lang\"], axis=1)\n",
    "new_mexico.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mexico.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a visualizar estos valores para poder analizarlos.\n",
    "\n",
    "Primero se analiza la cantidad de citas, favoritos, respuestas y retweets, observandolo como histogramas y sus valores ¿?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_interaction(df):\n",
    "    print(df[['quote_count', 'favorite_count', 'reply_count', 'retweet_count']].describe())\n",
    "    new_df = df[df['retweet_count'] > 50]\n",
    "    new_df[['quote_count', 'favorite_count', 'reply_count', 'retweet_count']].hist()\n",
    "\n",
    "hist_interaction(new_mexico)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se intenta contabilizar el resto de los valores donde se permite, como si el tweet es sensitivo, si es una cita o si tiene un url anexado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_booleans(df):\n",
    "    valores_T = 0\n",
    "    valores_F = 0\n",
    "    quoteable = 0\n",
    "    not_quotable = 0\n",
    "    url = 0\n",
    "    urlnt = 0\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        if df[\"possibly_sensitive\"][i]:\n",
    "            valores_T +=1\n",
    "        else:\n",
    "            valores_F +=1\n",
    "\n",
    "        if df[\"is_quote_status\"][i]:\n",
    "            quoteable += 1\n",
    "        else:\n",
    "            not_quotable += 1        \n",
    "\n",
    "        if df[\"entities_urls\"][i]:\n",
    "            url += 1\n",
    "        else:\n",
    "            urlnt += 1\n",
    "\n",
    "    plt.bar([\"Posible Sensitivo\",\"Posible no Sensitivo\"],[valores_T, valores_F])\n",
    "    plt.show()\n",
    "\n",
    "    plt.bar([\"Es Quote\",\"No Es Quote\"],[quoteable, not_quotable])\n",
    "    plt.show()\n",
    "\n",
    "    plt.bar([\"Url Anexado\",\"Url no Anexado\"],[url, urlnt])\n",
    "    plt.show()\n",
    "\n",
    "count_booleans(new_mexico)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a analizar los tweets si son informativos y si son humanitarios. Se separan en grupos según el valor 'conf'. Si 'conf' es 1, se grafican los valores como tal. Si 'conf' es menor a 1, se gráfica junto al promedio de conf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_and_human(df):\n",
    "    \n",
    "    #se copia la dataframe\n",
    "    df_info = df.copy()\n",
    "    df_human = df.copy()\n",
    "\n",
    "    #se borran los archivos NaN correspondientes\n",
    "    df_info = df_info[df_info['text_info'].notna()]\n",
    "    df_info = df_info[df_info['text_info_conf'].notna()]\n",
    "    df_human = df_human[df_human['text_human'].notna()]\n",
    "    df_human = df_human[df_human['text_human_conf'].notna()]\n",
    "\n",
    "    #se reinicia la index si necesario\n",
    "    df_info.reset_index(inplace=True, drop=True)\n",
    "    df_human.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #se crea nueva id según index\n",
    "    df_info[\"new_id\"] = df_info.index\n",
    "    df_human[\"new_id\"] = df_human.index\n",
    "\n",
    "    \n",
    "    #se crean dataframes para separar conf=1 con conf<1\n",
    "    info_confiable = df_info[[\"new_id\", \"id\", \"text_info\", \"text_info_conf\"]].copy()\n",
    "    info_casi_confiable = df_info[[\"new_id\", \"id\", \"text_info\", \"text_info_conf\"]].copy()\n",
    "\n",
    "    wanted = []\n",
    "    not_wanted = []\n",
    "\n",
    "    i = 0\n",
    "    while i < info_confiable.shape[0]:\n",
    "        #print(info_confiable[\"text_info_conf\"][i])\n",
    "        if info_confiable[\"text_info_conf\"][i] < 1:\n",
    "            not_wanted.append(info_confiable[\"new_id\"][i])\n",
    "        else:\n",
    "            wanted.append(info_confiable[\"new_id\"][i])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for j in not_wanted:\n",
    "        info_confiable = info_confiable.drop(info_confiable[info_confiable[\"new_id\"]==j].index)\n",
    "\n",
    "    for k in wanted:\n",
    "        info_casi_confiable = info_casi_confiable.drop(info_casi_confiable[info_casi_confiable[\"new_id\"]==k].index)\n",
    "\n",
    "\n",
    "    #se crean dataframes para separar conf=1 con conf<1\n",
    "    human_confiable = df_human[[\"new_id\", \"id\", \"text_human\", \"text_human_conf\"]].copy()\n",
    "    human_casi_confiable = df_human[[\"new_id\", \"id\", \"text_human\", \"text_human_conf\"]].copy()\n",
    "\n",
    "    hwanted = []\n",
    "    not_hwanted = []\n",
    "\n",
    "    i = 0\n",
    "    while i < human_confiable.shape[0]:\n",
    "        #print(info_confiable[\"text_info_conf\"][i])\n",
    "        if human_confiable[\"text_human_conf\"][i] < 1:\n",
    "            not_hwanted.append(human_confiable[\"new_id\"][i])\n",
    "        else:\n",
    "            hwanted.append(human_confiable[\"new_id\"][i])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for j in not_hwanted:\n",
    "        human_confiable = human_confiable.drop(human_confiable[human_confiable[\"new_id\"]==j].index)\n",
    "\n",
    "    for k in hwanted:\n",
    "        human_casi_confiable = human_casi_confiable.drop(human_casi_confiable[human_casi_confiable[\"new_id\"]==k].index)\n",
    "\n",
    "\n",
    "    valores_info_confiable = info_confiable[\"text_info\"].value_counts()\n",
    "\n",
    "    plt.bar(valores_info_confiable.index, valores_info_confiable.values)\n",
    "\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    valores_human_confiable = human_confiable[\"text_human\"].value_counts()\n",
    "\n",
    "    plt.bar(valores_human_confiable.index, valores_human_confiable.values)\n",
    "\n",
    "    for i, v in enumerate(valores_human_confiable.values):\n",
    "        plt.text(i, v + 1, str(v), ha=\"center\")\n",
    "\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"\")\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # calculate counts and means\n",
    "    counts = info_casi_confiable['text_info'].value_counts().sort_index()\n",
    "    means = info_casi_confiable.groupby('text_info')['text_info_conf'].mean().sort_index()\n",
    "\n",
    "    # plot the bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    counts.plot(kind='bar', color='blue', width=0.4, position=0, ax=ax)\n",
    "    means.plot(kind='bar', color='red', width=0.4, position=1, ax=ax)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_xlabel('Categorical Value')\n",
    "    ax.set_ylabel('Count / Mean Reliability')\n",
    "    ax.set_title('Count and Mean Reliability by Categorical Value')\n",
    "\n",
    "    # add value labels\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(i-0.2, count+0.1, str(count), color='black', fontweight='bold')\n",
    "    for i, mean in enumerate(means):\n",
    "        ax.text(i+0.2, mean+0.02, f'{mean:.2f}', color='black', ha='center')\n",
    "\n",
    "    # calculate counts and means\n",
    "    counts = human_casi_confiable['text_human'].value_counts().sort_index()\n",
    "    means = human_casi_confiable.groupby('text_human')['text_human_conf'].mean().sort_index()\n",
    "\n",
    "    # plot the bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    counts.plot(kind='bar', color='blue', width=0.4, position=0, ax=ax)\n",
    "    means.plot(kind='bar', color='red', width=0.4, position=1, ax=ax)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_xlabel('Categorical Value')\n",
    "    ax.set_ylabel('Count / Mean Reliability')\n",
    "    ax.set_title('Count and Mean Reliability by Categorical Value')\n",
    "\n",
    "    # add value labels\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(i-0.2, count+0.1, str(count), color='black', fontweight='bold')\n",
    "    for i, mean in enumerate(means):\n",
    "        ax.text(i+0.2, mean+0.02, f'{mean:.2f}', color='black', ha='center')\n",
    "\n",
    "info_and_human(new_mexico)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a observar el texto de cada tweet. Se contabilizan las palabras usadas para buscar las más repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(L, n):\n",
    "    words = []\n",
    "    for text in list(L): \n",
    "        for word in str(text).split():\n",
    "            #Aca se filtran las palabras que se repiten con mayusculas\n",
    "            wl = re.sub(r'[\\W_]+', '', word.lower())\n",
    "            # california. -> california\n",
    "            if word in ['US','USA', 'usa', 'Usa']:\n",
    "                words.append('USA')\n",
    "            \n",
    "            elif wl == \"\":   \n",
    "                pass\n",
    "        \n",
    "            elif word.lower()[:-2] == \"'s\":\n",
    "                words.append(wl[0:-1])\n",
    "\n",
    "            elif word.lower() != wl:\n",
    "                words.append(wl)\n",
    "    \n",
    "            else:\n",
    "                words.append(word.lower())\n",
    "\n",
    "    counter_words = Counter(words)\n",
    "    pairs = counter_words.most_common(n)\n",
    "    return pairs\n",
    "\n",
    "def filter_count_words(L,n):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('-')\n",
    "    stop_words.add('&amp;')\n",
    "    stop_words.add('|')\n",
    "    a=count_words(L,n)\n",
    "    filtered = []\n",
    "    for word in range(len(a)-1):\n",
    "        if a[word][0].lower() not in stop_words:\n",
    "            filtered.append(a[word])\n",
    "    return filtered\n",
    "\n",
    "def graficar_palabras(pares):\n",
    "    words = [par[0] for par in pares]\n",
    "    frequencies = [par[1] for par in pares]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(words, frequencies)\n",
    "    ax.set_xlabel('Palabras')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.set_title('Frecuencia de palabras')\n",
    "    ax.legend(['Frecuencia'])    \n",
    "    ax.tick_params(axis='x', labelrotation=90)  \n",
    "    fig.set_size_inches(15, 6)\n",
    "    plt.show()\n",
    "\n",
    "graficar_palabras(filter_count_words(mexico['text'], 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(df):\n",
    "    df_0 = df[df['retweet_count'] > 0]\n",
    "    df_0[['favorite_count', 'retweet_count']].plot(kind='box')\n",
    "    df_0[['quote_count', 'reply_count']].plot(kind='box')\n",
    "boxplots(mexico)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
